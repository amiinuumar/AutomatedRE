{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b25263-7994-4738-a660-0aadfd7e030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade scikit-learn==1.3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dac8c8b-6163-44f8-9c5f-dae45f28f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from joblib import dump, load\n",
    "import sys\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "# smote = SMOTE()\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee8d498f-4b15-4a22-b5f9-c150477f3da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abdul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/abdul/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/abdul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/abdul/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "# twelve test questions \n",
    "# url = 'https://docs.google.com/spreadsheets/d/18Sd81YVm8CTNTEsvs-lUOujExNg8Xw1MqEwAu-MGsNY/export?format=csv'\n",
    "\n",
    "#two other test questions\n",
    "url = 'https://docs.google.com/spreadsheets/d/1srQExpxj8Xw2kKHCpuI61U8ewJMyY6Mc/export?format=csv'\n",
    "\n",
    "\n",
    "# Read the CSV data into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "# df = df.iloc[[0]]  # Use the first row for testing\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map Treebank POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Map POS tag to a format accepted by WordNet lemmatizer.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to extract class-attribute mappings from the attribute string\n",
    "def extract_class_attribute_mapping(attribute_string):\n",
    "    class_attribute_mapping = {}\n",
    "    # Find all class-attribute groups in the format 'ClassName [attributes]'\n",
    "    class_attribute_groups = re.findall(r'(\\w+)\\s*\\[([^\\]]+)\\]', attribute_string)\n",
    "    for group in class_attribute_groups:\n",
    "        class_name, attributes = group\n",
    "        # Lemmatize the class name\n",
    "        class_name_lem = lemmatizer.lemmatize(class_name.lower(), pos='n')\n",
    "        attributes_list = [attr.strip() for attr in attributes.split(',')]\n",
    "        # Lemmatize attributes\n",
    "        attributes_lem = [lemmatizer.lemmatize(attr.lower(), pos='n') for attr in attributes_list]\n",
    "        class_attribute_mapping[class_name_lem] = attributes_lem\n",
    "    return class_attribute_mapping\n",
    "\n",
    "# Function to parse relationships between classes\n",
    "def parse_relationships(relationship_string):\n",
    "    relationships = []\n",
    "    for rel in relationship_string.split(','):\n",
    "        rel = rel.strip()\n",
    "        if 'and' in rel:\n",
    "            class_pair = tuple(map(str.strip, rel.split('and')))\n",
    "            # Lemmatize class names in relationships\n",
    "            class_pair_lem = tuple(lemmatizer.lemmatize(cls.lower(), pos='n') for cls in class_pair)\n",
    "            relationships.append(class_pair_lem)\n",
    "    return relationships\n",
    "\n",
    "# Global sentence counter\n",
    "global_sentence_counter = 0\n",
    "\n",
    "# Function to tag words in the problem text as 'Class', 'Attribute', or 'Other'\n",
    "def tag_problem_classes_and_attributes(problem_number, problem, class_attribute_mapping, class_list_lem, relationships):\n",
    "    global global_sentence_counter  # Reference the global sentence counter\n",
    "\n",
    "    # Lists to store the tagging results\n",
    "    problems = []\n",
    "    problem_numbers = []\n",
    "    sentence_numbers = []\n",
    "    sent_list = []\n",
    "    word_list = []\n",
    "    pos_list = []\n",
    "    tag_list = []\n",
    "    class_related_list = []\n",
    "    class_r_list = []\n",
    "\n",
    "    # Tokenize the problem into sentences\n",
    "    sentences = nltk.sent_tokenize(problem)\n",
    "    for sentence in sentences:\n",
    "        global_sentence_counter += 1  # Increment global sentence counter\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Remove punctuation from words\n",
    "        words = [word for word in words if word.lower() not in string.punctuation]\n",
    "        # POS tagging\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "        lemmatized_words = []\n",
    "        # Lemmatize words\n",
    "        for word, pos in pos_tags:\n",
    "            wordnet_pos = get_wordnet_pos(pos)\n",
    "            lemmatized_word = lemmatizer.lemmatize(word.lower(), pos=wordnet_pos)\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "\n",
    "        lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "\n",
    "        for word, lemmatized_word, pos in zip(words, lemmatized_words, [p for w, p in pos_tags]):\n",
    "            problem_numbers.append(problem_number)\n",
    "            sentence_numbers.append(f\"Sentence: {global_sentence_counter}\")\n",
    "            problems.append(problem)\n",
    "            sent_list.append(lemmatized_sentence)\n",
    "            word_list.append(lemmatized_word)\n",
    "            pos_list.append(pos)\n",
    "\n",
    "            tag = 'Other'\n",
    "            found_class = 'Other'\n",
    "            found_relationship = 'Other'\n",
    "\n",
    "            # Check if the word is an attribute\n",
    "            attribute_found = False\n",
    "            for class_name, attributes in class_attribute_mapping.items():\n",
    "                if lemmatized_word in attributes:\n",
    "                    tag = \"Attribute\"\n",
    "                    found_class = class_name\n",
    "                    attribute_found = True\n",
    "                    break\n",
    "\n",
    "            # If not an attribute, check if the word is a class\n",
    "            if not attribute_found:\n",
    "                if lemmatized_word in class_list_lem:\n",
    "                    tag = \"Class\"\n",
    "                    found_class = lemmatized_word\n",
    "                    # Find related class from relationships\n",
    "                    for rel in relationships:\n",
    "                        if found_class in rel:\n",
    "                            found_relationship = rel[1] if rel[0] == found_class else rel[0]\n",
    "                            break\n",
    "\n",
    "            class_related_list.append(found_class if found_class != 'Other' else \"\")\n",
    "            class_r_list.append(found_relationship if found_relationship != 'Other' else \"\")\n",
    "            tag_list.append(tag)\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame({\n",
    "        'Problem_Number': problem_numbers,\n",
    "        'Sentence #': sentence_numbers,\n",
    "        'Problem': problems,\n",
    "        'Sentence': sent_list,\n",
    "        'Word': word_list,\n",
    "        'POS': pos_list,\n",
    "        'Tag': tag_list,\n",
    "        'Class_Related': class_related_list,\n",
    "        'Class_R': class_r_list\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "tagged_data_list = []\n",
    "\n",
    "# Iterate over the DataFrame rows to process each problem\n",
    "for index, row in df.iterrows():\n",
    "    problem_number = index + 1\n",
    "    problem_text = row['Problem']\n",
    "    attribute_string = row['Atributes']  # Column name 'Atributes'\n",
    "    relationship_string = row.get('Relationship', '')\n",
    "    class_list_string = row['Class']\n",
    "\n",
    "    # Extract and lemmatize class names\n",
    "    class_list = [cls.strip() for cls in class_list_string.split(',')]\n",
    "    class_list_lem = [lemmatizer.lemmatize(cls.lower(), pos='n') for cls in class_list]\n",
    "\n",
    "    # Extract class-attribute mappings and relationships\n",
    "    class_attribute_mapping = extract_class_attribute_mapping(attribute_string)\n",
    "    relationships = parse_relationships(relationship_string)\n",
    "\n",
    "    # Tag the problem text\n",
    "    tagged_df = tag_problem_classes_and_attributes(\n",
    "        problem_number, problem_text, class_attribute_mapping, class_list_lem, relationships)\n",
    "    tagged_data_list.append(tagged_df)\n",
    "\n",
    "# Concatenate all tagged data into a single DataFrame\n",
    "final_tagged_df = pd.concat(tagged_data_list, ignore_index=True)\n",
    "\n",
    "# Reorder and clean up the columns\n",
    "final_tagged_df = final_tagged_df[['Problem_Number', 'Sentence #', 'Problem', 'Sentence', 'Word', 'POS', 'Tag', 'Class_Related', 'Class_R']]\n",
    "final_tagged_df['Class_Related'] = final_tagged_df['Class_Related'].replace('', 'Other')\n",
    "final_tagged_df['Class_R'] = final_tagged_df['Class_R'].replace('', 'Other')\n",
    "\n",
    "# Display the DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3102a026-eeb9-45b0-9154-7ab9b6069ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem_Number</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Problem</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Class_Related</th>\n",
       "      <th>Class_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>The Stroke Recovery System shall have a compre...</td>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>The Stroke Recovery System shall have a compre...</td>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>stroke</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>The Stroke Recovery System shall have a compre...</td>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>recovery</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>The Stroke Recovery System shall have a compre...</td>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>system</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>The Stroke Recovery System shall have a compre...</td>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>shall</td>\n",
       "      <td>MD</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 23</td>\n",
       "      <td>Researcher is empowered to view finding aids f...</td>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>promote</td>\n",
       "      <td>VBG</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 23</td>\n",
       "      <td>Researcher is empowered to view finding aids f...</td>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>collaboration</td>\n",
       "      <td>NN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 23</td>\n",
       "      <td>Researcher is empowered to view finding aids f...</td>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>usability</td>\n",
       "      <td>NN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 23</td>\n",
       "      <td>Researcher is empowered to view finding aids f...</td>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 23</td>\n",
       "      <td>Researcher is empowered to view finding aids f...</td>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>reliability</td>\n",
       "      <td>NN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Problem_Number    Sentence #  \\\n",
       "0                 1   Sentence: 1   \n",
       "1                 1   Sentence: 1   \n",
       "2                 1   Sentence: 1   \n",
       "3                 1   Sentence: 1   \n",
       "4                 1   Sentence: 1   \n",
       "..              ...           ...   \n",
       "439               2  Sentence: 23   \n",
       "440               2  Sentence: 23   \n",
       "441               2  Sentence: 23   \n",
       "442               2  Sentence: 23   \n",
       "443               2  Sentence: 23   \n",
       "\n",
       "                                               Problem  \\\n",
       "0    The Stroke Recovery System shall have a compre...   \n",
       "1    The Stroke Recovery System shall have a compre...   \n",
       "2    The Stroke Recovery System shall have a compre...   \n",
       "3    The Stroke Recovery System shall have a compre...   \n",
       "4    The Stroke Recovery System shall have a compre...   \n",
       "..                                                 ...   \n",
       "439  Researcher is empowered to view finding aids f...   \n",
       "440  Researcher is empowered to view finding aids f...   \n",
       "441  Researcher is empowered to view finding aids f...   \n",
       "442  Researcher is empowered to view finding aids f...   \n",
       "443  Researcher is empowered to view finding aids f...   \n",
       "\n",
       "                                              Sentence           Word  POS  \\\n",
       "0    the stroke recovery system shall have a compre...            the   DT   \n",
       "1    the stroke recovery system shall have a compre...         stroke  NNP   \n",
       "2    the stroke recovery system shall have a compre...       recovery  NNP   \n",
       "3    the stroke recovery system shall have a compre...         system  NNP   \n",
       "4    the stroke recovery system shall have a compre...          shall   MD   \n",
       "..                                                 ...            ...  ...   \n",
       "439  the extensive requirement be design to fulfil ...        promote  VBG   \n",
       "440  the extensive requirement be design to fulfil ...  collaboration   NN   \n",
       "441  the extensive requirement be design to fulfil ...      usability   NN   \n",
       "442  the extensive requirement be design to fulfil ...            and   CC   \n",
       "443  the extensive requirement be design to fulfil ...    reliability   NN   \n",
       "\n",
       "       Tag Class_Related Class_R  \n",
       "0    Other         Other   Other  \n",
       "1    Other         Other   Other  \n",
       "2    Other         Other   Other  \n",
       "3    Other         Other   Other  \n",
       "4    Other         Other   Other  \n",
       "..     ...           ...     ...  \n",
       "439  Other         Other   Other  \n",
       "440  Other         Other   Other  \n",
       "441  Other         Other   Other  \n",
       "442  Other         Other   Other  \n",
       "443  Other         Other   Other  \n",
       "\n",
       "[444 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79cbfbe9-c2ad-41c1-849f-985576b216fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing a single requirment problem for prediction\n",
    "\n",
    "from typing_extensions import final\n",
    "\n",
    "def process_requirments(one_question):\n",
    "     #Iterate over the rows in the DataFrame\n",
    "    df = one_question\n",
    "    tagged_data_list = []\n",
    "\n",
    "    # Iterate over the rows in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        problem_number = index + 1\n",
    "        problem_text = row['Problem']\n",
    "        attribute_string = row['Atributes']  # Corrected column name\n",
    "        relationship_string = row.get('Relationship', '')\n",
    "\n",
    "        class_attribute_mapping = extract_class_attribute_mapping(attribute_string)\n",
    "        relationships = parse_relationships(relationship_string)\n",
    "\n",
    "        tagged_df = tag_problem_classes_and_attributes(problem_number, problem_text, class_attribute_mapping, relationships)\n",
    "        tagged_data_list.append(tagged_df)\n",
    "\n",
    "    # Concatenate all the tagged data into a single DataFrame\n",
    "    final_tagged_df = pd.concat(tagged_data_list, ignore_index=True)\n",
    "\n",
    "    # Reorder the columns\n",
    "    final_tagged_df = final_tagged_df[['Problem_Number', 'Sentence #', 'Problem', 'Sentence', 'Word', 'POS', 'Tag', 'Class_Related', 'Class_R']]\n",
    "\n",
    "\n",
    "    final_tagged_df['Class_Related'] = final_tagged_df['Class_Related'].replace('', 'Other')\n",
    "    final_tagged_df['Class_R'] = final_tagged_df['Class_R'].replace('', 'Other')\n",
    "\n",
    "    # final_tagged_df = final_tagged_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    return final_tagged_df\n",
    "\n",
    "    # Save to a new CSV file or display the DataFrame\n",
    "    # final_tagged_df.to_csv('tagged_problems.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65003c3-c391-4657-8e63-e9826e654b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_question = '''\n",
    "# #\n",
    "# sma ekjgkw wgewflewjgwlkow wfopewkfpw\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ce514-1f3e-437e-be54-cd6e42716156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_tagged_df = process_requirments(one_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "989a8aea-6a8b-4ef1-86fb-27d35fbed0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import pandas as pd\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Assume final_tagged_df is already created from previous steps\n",
    "# # If not, make sure to run the code to generate final_tagged_df\n",
    "\n",
    "# # Download NLTK stopwords corpus\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Create a set of English stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Convert words to lowercase for case-insensitive comparison\n",
    "# final_tagged_df['Word_lower'] = final_tagged_df['Word'].str.lower()\n",
    "\n",
    "# # Create a mask to filter out stop words\n",
    "# mask = ~final_tagged_df['Word_lower'].isin(stop_words)\n",
    "\n",
    "# # Filter the DataFrame\n",
    "# filtered_df = final_tagged_df[mask].copy()\n",
    "\n",
    "# # Drop the temporary column\n",
    "# filtered_df.drop(columns=['Word_lower'], inplace=True)\n",
    "\n",
    "# # Optional: Reset index if needed\n",
    "# filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Optional: Compare the number of 'Other' tags\n",
    "# original_other_count = final_tagged_df[final_tagged_df['Tag'] == 'Other'].shape[0]\n",
    "# filtered_other_count = filtered_df[filtered_df['Tag'] == 'Other'].shape[0]\n",
    "\n",
    "# print(f\"Original 'Other' tags count: {original_other_count}\")\n",
    "# print(f\"Filtered 'Other' tags count: {filtered_other_count}\")\n",
    "\n",
    "\n",
    "# # Display the filtered DataFrame\n",
    "# print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "878f1cea-b15c-4b7f-9e24-be3b4f2dccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39ebf69d-d7eb-4680-80d2-9ba2e58e1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tag_model = load('tag_model.joblib')\n",
    "class_related_model = load('class_related_model.joblib')\n",
    "class_r_model = load('class_r_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09f5c8f3-0ef2-45dc-ab80-e26314dc25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_tagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d12f3ec-047b-47ae-8458-2ba6d8791192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy_score from scikit-learn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list to hold each classification report's results\n",
    "results = []\n",
    "Dataframe_results = pd.DataFrame()  # Initialize an empty DataFrame to hold all the predicted data\n",
    "\n",
    "def predict_class_attribute(parsed_data, rq_label, problem_number):\n",
    "    global Dataframe_results\n",
    "    \n",
    "    # Model to predict Class_Attribute\n",
    "    predicted_data = parsed_data[['Sentence', 'Word', 'POS']].copy()\n",
    "    predicted_data.loc[:, 'Tag'] = tag_model.predict(parsed_data[['Sentence', 'Word', 'POS']])\n",
    "    \n",
    "    # Generate classification report for Class_Attribute\n",
    "    report = classification_report(parsed_data['Tag'], predicted_data['Tag'], output_dict=True, zero_division=1)\n",
    "    \n",
    "    # Calculate accuracy for Class_Attribute\n",
    "    accuracy_attr = accuracy_score(parsed_data['Tag'], predicted_data['Tag'])\n",
    "    \n",
    "    # Predict Class_Related using Predicted_Tag\n",
    "    predicted_data.loc[:, 'Class_Related'] = class_related_model.predict(predicted_data[['Sentence', 'Word', 'POS', 'Tag']])\n",
    "    \n",
    "    # Generate classification report for Class_Related\n",
    "    report_related = classification_report(parsed_data['Class_Related'], predicted_data['Class_Related'], output_dict=True, zero_division=1)\n",
    "    \n",
    "    # Calculate accuracy for Class_Related\n",
    "    accuracy_related = accuracy_score(parsed_data['Class_Related'], predicted_data['Class_Related'])\n",
    "    \n",
    "    # Predict Class_R using Predicted_Tag and Predicted_Class_Related\n",
    "    predicted_data.loc[:, 'Class_R'] = class_r_model.predict(predicted_data[['Sentence', 'Word', 'POS', 'Tag', 'Class_Related']])\n",
    "    \n",
    "    # Add a column with the problem number for this iteration\n",
    "    predicted_data['Problem_Number'] = problem_number\n",
    "    \n",
    "    # Concatenate the current problem's predicted data to the global DataFrame\n",
    "    Dataframe_results = pd.concat([Dataframe_results, predicted_data], ignore_index=True)\n",
    "    \n",
    "    # Generate classification report for Class_R\n",
    "    report_r = classification_report(parsed_data['Class_R'], predicted_data['Class_R'], output_dict=True, zero_division=1)\n",
    "    \n",
    "    # Calculate accuracy for Class_R\n",
    "    accuracy_r = accuracy_score(parsed_data['Class_R'], predicted_data['Class_R'])\n",
    "    \n",
    "    # Append the metrics for each class (Attribute, Class_Related, Class_R) to the results list\n",
    "    results.append({\n",
    "        'rq': rq_label,\n",
    "        'Class_Attribute_Accuracy': accuracy_attr,\n",
    "        'Class_Attribute_Precision_Macro': report['macro avg']['precision'],\n",
    "        'Class_Attribute_Recall_Macro': report['macro avg']['recall'],\n",
    "        'Class_Attribute_F1-Macro': report['macro avg']['f1-score'],\n",
    "        \n",
    "        'Class_Related_Accuracy': accuracy_related,\n",
    "        'Class_Related_Precision_Macro': report_related['macro avg']['precision'],\n",
    "        'Class_Related_Recall_Macro': report_related['macro avg']['recall'],\n",
    "        'Class_Related_F1-Macro': report_related['macro avg']['f1-score'],\n",
    "        \n",
    "        'Class_R_Accuracy': accuracy_r,\n",
    "        'Class_R_Precision_Macro': report_r['macro avg']['precision'],\n",
    "        'Class_R_Recall_Macro': report_r['macro avg']['recall'],\n",
    "        'Class_R_F1-Macro': report_r['macro avg']['f1-score']\n",
    "    })\n",
    "\n",
    "# Loop over all unique problem numbers in the DataFrame\n",
    "unique_problem_numbers = final_tagged_df['Problem_Number'].unique()\n",
    "\n",
    "# Iterate over each unique problem number\n",
    "for problem_number in unique_problem_numbers:\n",
    "    # Select rows with the current problem number\n",
    "    parsed_data = final_tagged_df[final_tagged_df['Problem_Number'] == problem_number]\n",
    "    \n",
    "    # Label for the current problem\n",
    "    rq_label = f\"rq{problem_number}\"\n",
    "    \n",
    "    # Call the prediction function for the current problem\n",
    "    predict_class_attribute(parsed_data, rq_label, problem_number)\n",
    "\n",
    "# Convert the results list into a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Transpose the DataFrame so that each requirement (rq1, rq2, etc.) becomes the header\n",
    "df_results_transposed = df_results.set_index('rq').transpose()\n",
    "\n",
    "# Display the transposed DataFrame (optional)\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Transposed Classification Report Results\", dataframe=df_results_transposed)\n",
    "\n",
    "# Save Dataframe_results for future analysis\n",
    "Dataframe_results.to_csv('predicted_results.csv', index=False)\n",
    "\n",
    "# Save the classification report results for future analysis\n",
    "df_results.to_csv('classification_report_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48ae57d2-dd3d-433e-a675-a52eb15b937c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rq</th>\n",
       "      <th>rq1</th>\n",
       "      <th>rq2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class_Attribute_Accuracy</th>\n",
       "      <td>0.929688</td>\n",
       "      <td>0.888298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Attribute_Precision_Macro</th>\n",
       "      <td>0.596084</td>\n",
       "      <td>0.515184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Attribute_Recall_Macro</th>\n",
       "      <td>0.437442</td>\n",
       "      <td>0.835455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Attribute_F1-Macro</th>\n",
       "      <td>0.480998</td>\n",
       "      <td>0.508516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Related_Accuracy</th>\n",
       "      <td>0.929688</td>\n",
       "      <td>0.888298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Related_Precision_Macro</th>\n",
       "      <td>0.688840</td>\n",
       "      <td>0.510911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Related_Recall_Macro</th>\n",
       "      <td>0.503878</td>\n",
       "      <td>0.732533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_Related_F1-Macro</th>\n",
       "      <td>0.207237</td>\n",
       "      <td>0.252340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_R_Accuracy</th>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.877660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_R_Precision_Macro</th>\n",
       "      <td>0.425714</td>\n",
       "      <td>0.435307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_R_Recall_Macro</th>\n",
       "      <td>0.713129</td>\n",
       "      <td>0.622456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class_R_F1-Macro</th>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.060131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rq                                    rq1       rq2\n",
       "Class_Attribute_Accuracy         0.929688  0.888298\n",
       "Class_Attribute_Precision_Macro  0.596084  0.515184\n",
       "Class_Attribute_Recall_Macro     0.437442  0.835455\n",
       "Class_Attribute_F1-Macro         0.480998  0.508516\n",
       "Class_Related_Accuracy           0.929688  0.888298\n",
       "Class_Related_Precision_Macro    0.688840  0.510911\n",
       "Class_Related_Recall_Macro       0.503878  0.732533\n",
       "Class_Related_F1-Macro           0.207237  0.252340\n",
       "Class_R_Accuracy                 0.957031  0.877660\n",
       "Class_R_Precision_Macro          0.425714  0.435307\n",
       "Class_R_Recall_Macro             0.713129  0.622456\n",
       "Class_R_F1-Macro                 0.140845  0.060131"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "270cface-0652-4e5b-8ae9-dd99349b7a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Class_Related</th>\n",
       "      <th>Class_R</th>\n",
       "      <th>Problem_Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>stroke</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>recovery</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>system</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the stroke recovery system shall have a compre...</td>\n",
       "      <td>shall</td>\n",
       "      <td>MD</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>promote</td>\n",
       "      <td>VBG</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>collaboration</td>\n",
       "      <td>NN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>usability</td>\n",
       "      <td>NN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>the extensive requirement be design to fulfil ...</td>\n",
       "      <td>reliability</td>\n",
       "      <td>NN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence           Word  POS  \\\n",
       "0    the stroke recovery system shall have a compre...            the   DT   \n",
       "1    the stroke recovery system shall have a compre...         stroke  NNP   \n",
       "2    the stroke recovery system shall have a compre...       recovery  NNP   \n",
       "3    the stroke recovery system shall have a compre...         system  NNP   \n",
       "4    the stroke recovery system shall have a compre...          shall   MD   \n",
       "..                                                 ...            ...  ...   \n",
       "439  the extensive requirement be design to fulfil ...        promote  VBG   \n",
       "440  the extensive requirement be design to fulfil ...  collaboration   NN   \n",
       "441  the extensive requirement be design to fulfil ...      usability   NN   \n",
       "442  the extensive requirement be design to fulfil ...            and   CC   \n",
       "443  the extensive requirement be design to fulfil ...    reliability   NN   \n",
       "\n",
       "       Tag Class_Related Class_R  Problem_Number  \n",
       "0    Other         Other   Other               1  \n",
       "1    Other         Other   Other               1  \n",
       "2    Other         Other   Other               1  \n",
       "3    Other         Other   Other               1  \n",
       "4    Other         Other   Other               1  \n",
       "..     ...           ...     ...             ...  \n",
       "439  Other         Other   Other               2  \n",
       "440  Other         Other   Other               2  \n",
       "441  Other         Other   Other               2  \n",
       "442  Other         Other   Other               2  \n",
       "443  Other         Other   Other               2  \n",
       "\n",
       "[444 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataframe_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c0102c3-9093-4602-b1d1-bc0e0da8b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_transposed .to_csv('RQ_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3781dd49-1d6e-4d84-a6ea-38807dc79592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Problem Number: 1\n",
      "\n",
      "Classes: ['patient', 'user', 'application']\n",
      "Attributes: ['history', 'patient', 'treatment', 'exercise']\n",
      "\n",
      "Attributes associated with classes:\n",
      "- User: Exercise\n",
      "\n",
      "Relationships between classes:\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing Problem Number: 2\n",
      "\n",
      "Classes: ['researcher', 'resource', 'application', 'user', 'account', 'entry', 'file', 'manager', 'archive', 'space', 'project']\n",
      "Attributes: ['password', 'url', 'focus', 'source', 'id', 'description', 'location']\n",
      "\n",
      "Attributes associated with classes:\n",
      "- Application: Password, Url\n",
      "- User: Password, Url\n",
      "- Account: Password, Url\n",
      "- Manager: Location, Description\n",
      "\n",
      "Relationships between classes:\n",
      "- Application compound User\n",
      "- Entry compound User\n",
      "- Researcher compound Resource\n",
      "- User compound Account\n",
      "- Space compound Project\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Extend the Token class to include custom attributes if not already set\n",
    "from spacy.tokens import Token\n",
    "\n",
    "if not Token.has_extension('is_class'):\n",
    "    Token.set_extension('is_class', default=False)\n",
    "if not Token.has_extension('is_attribute'):\n",
    "    Token.set_extension('is_attribute', default=False)\n",
    "\n",
    "def parse_tagged_data(df):\n",
    "    \"\"\"\n",
    "    Parses the DataFrame and groups words by sentence text (not sentence number).\n",
    "    \"\"\"\n",
    "    sentences = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence_text = str(row['Sentence'])\n",
    "        word = str(row['Word'])\n",
    "        tag = str(row['Tag'])\n",
    "        sentences[sentence_text].append({'word': word, 'tag': tag})\n",
    "    return sentences\n",
    "\n",
    "def process_sentences(sentences):\n",
    "    class_attributes = defaultdict(set)\n",
    "    class_relationships = set()\n",
    "\n",
    "    for sentence_text, tokens in sentences.items():\n",
    "        # Collect all classes and attributes in the sentence\n",
    "        classes = [token['word'] for token in tokens if token['tag'] == 'Class']\n",
    "        attributes = [token['word'] for token in tokens if token['tag'] == 'Attribute']\n",
    "\n",
    "        # Create relationships between classes and attributes\n",
    "        for cls in classes:\n",
    "            for attr in attributes:\n",
    "                class_attributes[cls.capitalize()].add(attr.capitalize())\n",
    "\n",
    "        # Reconstruct the sentence text for processing in spaCy\n",
    "        sentence_words = [token['word'] for token in tokens]\n",
    "        reconstructed_sentence = ' '.join(sentence_words)\n",
    "        doc = nlp(reconstructed_sentence)\n",
    "\n",
    "        # For each class, check if there are relationships with other classes in the sentence\n",
    "        for token in doc:\n",
    "            if token.text.lower() in [cls.lower() for cls in classes]:\n",
    "                subject_class = token.text.capitalize()\n",
    "\n",
    "                # 1. Look for verbs connected to this class\n",
    "                for child in token.children:\n",
    "                    if child.pos_ == 'VERB':\n",
    "                        verb = child\n",
    "                        # Check for object classes connected to the verb\n",
    "                        for obj in verb.children:\n",
    "                            if obj.text.lower() in [cls.lower() for cls in classes] and obj != token:\n",
    "                                object_class = obj.text.capitalize()\n",
    "                                relationship = (subject_class, verb.lemma_, object_class)\n",
    "                                class_relationships.add(relationship)\n",
    "\n",
    "                # 2. Look for prepositions linking this class to others\n",
    "                for prep in [child for child in token.children if child.dep_ == 'prep']:\n",
    "                    for obj in prep.children:\n",
    "                        if obj.text.lower() in [cls.lower() for cls in classes]:\n",
    "                            relationship = (\n",
    "                                subject_class,\n",
    "                                prep.text,\n",
    "                                obj.text.capitalize()\n",
    "                            )\n",
    "                            class_relationships.add(relationship)\n",
    "\n",
    "                # 3. Look for conjunctions (e.g., \"Project and Group\")\n",
    "                for conjunct in token.conjuncts:\n",
    "                    if conjunct.text.lower() in [cls.lower() for cls in classes] and subject_class != conjunct.text.capitalize():\n",
    "                        relationship = (\n",
    "                            subject_class,\n",
    "                            \"and\",\n",
    "                            conjunct.text.capitalize()\n",
    "                        )\n",
    "                        class_relationships.add(relationship)\n",
    "\n",
    "                # 4. Check for compound nouns (e.g., \"funding group\")\n",
    "                if token.dep_ == 'compound':\n",
    "                    compound_head = token.head\n",
    "                    if compound_head.text.lower() in [cls.lower() for cls in classes] and subject_class != compound_head.text.capitalize():\n",
    "                        relationship = (\n",
    "                            subject_class,\n",
    "                            \"compound\",\n",
    "                            compound_head.text.capitalize()\n",
    "                        )\n",
    "                        class_relationships.add(relationship)\n",
    "\n",
    "    # Remove redundant relationships (e.g., \"Community compound Community\")\n",
    "    class_relationships = {\n",
    "        (subj, rel, obj) for subj, rel, obj in class_relationships if subj != obj\n",
    "    }\n",
    "\n",
    "    return class_attributes, class_relationships\n",
    "\n",
    "def list_classes_attributes_and_relationships():\n",
    "    # Get all unique problem numbers\n",
    "    unique_problem_numbers = Dataframe_results['Problem_Number'].unique()\n",
    "\n",
    "    # Iterate over each problem number\n",
    "    for problem_number in unique_problem_numbers:\n",
    "        print(f\"Processing Problem Number: {problem_number}\")\n",
    "        \n",
    "        # Filter the data for the current problem, considering only rows where Tag is 'Class' or 'Attribute'\n",
    "        problem_data = Dataframe_results[\n",
    "            (Dataframe_results['Problem_Number'] == problem_number) &\n",
    "            (Dataframe_results['Tag'].isin(['Class', 'Attribute']))\n",
    "        ]\n",
    "        \n",
    "        # Parse the data into sentences\n",
    "        sentences = parse_tagged_data(problem_data)\n",
    "        \n",
    "        # Process the sentences to get class attributes and relationships\n",
    "        class_attributes, class_relationships = process_sentences(sentences)\n",
    "        \n",
    "        # Get the words tagged as 'Class'\n",
    "        class_words = problem_data[problem_data['Tag'] == 'Class']['Word'].unique().tolist()\n",
    "        \n",
    "        # Get the words tagged as 'Attribute'\n",
    "        attribute_words = problem_data[problem_data['Tag'] == 'Attribute']['Word'].unique().tolist()\n",
    "        \n",
    "        # Extract class-related and class-r relationships\n",
    "        class_related_relationships = problem_data[problem_data['Class_Related'] != problem_data['Word']]\n",
    "        class_related_words = list(zip(class_related_relationships['Class_Related'], class_related_relationships['Word']))\n",
    "\n",
    "        # Exclude relationships where Class_R is 'Other'\n",
    "        class_r_relationships = problem_data[\n",
    "            (problem_data['Class_R'] != problem_data['Word']) & (problem_data['Class_R'] != 'Other')\n",
    "        ]\n",
    "        class_r_words = list(zip(class_r_relationships['Class_R'], class_r_relationships['Word']))\n",
    "        \n",
    "        # Output the classes and attributes\n",
    "        print(f\"\\nClasses: {class_words}\")\n",
    "        print(f\"Attributes: {attribute_words}\")\n",
    "        \n",
    "        # Output the Class_Related relationships\n",
    "#         print(\"\\nClass_Related relationships (Class -> Attribute):\")\n",
    "#         for class_word, attribute_word in class_related_words:\n",
    "#             print(f\"- {class_word} -> {attribute_word}\")\n",
    "\n",
    "#         # Output the Class_R relationships, excluding 'Other'\n",
    "#         print(\"\\nClass_R relationships (Class -> Class):\")\n",
    "#         for class_word, related_class_word in class_r_words:\n",
    "#             print(f\"- {class_word} -> {related_class_word}\")\n",
    "        \n",
    "        # Output the attributes associated with each class\n",
    "        print(\"\\nAttributes associated with classes:\")\n",
    "        for cls, attrs in class_attributes.items():\n",
    "            print(f\"- {cls}: {', '.join(attrs)}\")\n",
    "        \n",
    "        # Output the relationships between classes\n",
    "        print(\"\\nRelationships between classes:\")\n",
    "        for subj_class, verb, obj_class in class_relationships:\n",
    "            print(f\"- {subj_class} {verb} {obj_class}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        # Print a separator between each problem\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Call the function to process the DataFrame and list classes, attributes, and relationships\n",
    "list_classes_attributes_and_relationships()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776e19b-8540-4244-ae26-63f1c990b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fc9ba-2fe9-4a18-8cab-9a82f314146d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7e79a-c658-426e-9dbb-9ba8e937a605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c165a7-e33d-4f2b-845f-bd5af07edb43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
